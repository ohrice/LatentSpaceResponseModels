\documentclass[12pt]{article}

\usepackage[margin=1.25in]{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{ mathrsfs }

\renewcommand{\baselinestretch}{1.1}\pagestyle{fancy}
\pagestyle{fancy}
\usepackage{amsfonts}
\usepackage{bbold}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb,amsmath}
\usepackage{textpos}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {images/} }
\newcommand{\V}{\mathbb{V}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cov}{\textnormal{cov}}
\newcommand{\tx}[1]{\textnormal{#1}}
\usepackage{graphicx}
\setlength{\parindent}{0pt}

\begin{document}
	
	\begin{center}
		Description of Cluster Model
	\end{center}
	
\textbf{Response model:} Let $X_{ik}$ denote the response of respondent $k$ on question $i$. Then the logit model is as follows:

$$\tx{logit}(P(X_{ik}=1))=\theta_k+\beta_i-||z_k-w_i||_2^2$$ Where $\theta_k$ is the respondent's skill, $\beta_i$ is the item's difficulty, and $z_k$ and $w_i$ are the latent space positions of the respondent and the item respectively in two dimensional Euclidean space, where the closer they are in Euclidean space the larger the probability of the student correctly scoring on the item.\\


\textbf{Priors using Gaussian Mixture Model for Clustering}:

$$z_k,w_i\sim\sum_{g=1}^{G}\lambda_{g}MVN_d(\mu_{g},\sigma_{g}I_d)$$



$$\lambda\sim \tx{Dirichlet}(\nu)$$ $$\forall g:\,\,\sigma_{g}\sim \tx{InvGamma}(\gamma_{shape},\gamma_{scale})$$ $$\forall g:\,\,\mu_{g}\sim MVN_d(0,\omega^2I_d)$$
$$\theta\sim N(0,\sigma_\theta^2I_{n_z}),\beta\sim N(0,\sigma_\beta^2 I_{n_w})$$
$$\sigma_\theta^2\sim \tx{Inv-Gamma}(a,b);\tx{  }\sigma_\beta\tx{ kept fixed}$$
The hyper-parameters can be tuned as desired to the given dataset.\\
 
 \textbf{Priors using Dirichlet Model for Clustering} Here, we cluster only on means (as opposed to on means and variance) where the means are drawn from a distribution which itself is drawn from a Dirichlet process. As draws from Dirichlet processes are discrete, there will be a finite number of means; we take the size of the support of the distribution to be the number of clusters. This way, the number of clusters is random and can be estimated within the MCMC algorithm.
 
 $$z_k\sim MVN_d(\mu_{k},\sigma_{zw} I_d),w_i\sim MVN_d(\mu_{i},\sigma_{zw} I_d)$$ where $$\mu_k,\mu_i\sim P; P\sim\tx{Dirichlet}(H(\omega),\alpha)$$ The base distribution $H$ can be parameterized by $\lambda$. We will take $$H(\omega)=N(0,\omega^2I_d)$$ similar to the above. The parameter $\alpha$ is related to the size of the support of the distribution $P$ or equivalently the number of clusters; $\alpha\rightarrow 0$ there will be only one cluster and as $\alpha\rightarrow\infty$ the distribution $P$ will be continuous so the number of clusters will equal the number of datapoints. Then similarly to explicit mixture model, we set  $$\theta\sim N(0,\sigma_\theta^2I_{n_z}),\beta\sim N(0,\sigma_\beta^2 I_{n_w})$$
 $$\sigma_{zw},\sigma_\theta^2\sim \tx{Inv-Gamma}(a,b);\tx{  }\sigma_\beta\tx{ kept fixed}$$
 
The hyper-parameters can be tuned as desired to the given dataset.\\

\textbf{Prior Using Inverse Chi-Squared for $\sigma_g^2$}\\

If you choose the prior $\sigma_g^2\sim \sigma_0^2\tx{Inv}\chi^2_{\alpha}$, then the Gibbs conditional posterior can be found through the following derivation, recalling that $m_g=\sum_{i:K_ig}1$, $s_g^2=\frac{1}{2}\sum_{i:K_i=g}(z_i-\mu_g)^2$. $$P(\sigma_g^2|\tx{others})\propto P(\tx{others}|\sigma_g^2)P(\sigma_g^2)$$ $$\propto\prod_{i:K_i=g}\frac{1}{\sigma_g}e^{-(z_i-\mu_g)^2/(2\sigma_g^2)}\,\cdot\, \sigma_g^{-2(\alpha/2-1)}e^{-\sigma_0^2/(2\sigma_g^2)}$$ $$=\sigma_g^{2(-\alpha/2-1-m_g)}e^{-(\sigma_0^2+\sum_{i:K_i=g}(z_i-\mu_g)^2)/(2\sigma_g^2)}$$ $$=\sigma_g^{2(-(\alpha+2m_g)/2-1)}e^{-(\sigma_0^2+2s_g^2)/(2\sigma_g^2)}$$ $$\propto (\sigma_0^2+2s_g^2)\tx{Inv}\chi^2_{\alpha+2m_g}$$\\
 

\textbf{Proposal distributions.} For $z,w,\beta,$ and  $\theta$, proposed values are drawn from normal distributions centered around the previous value with tunable proposal standard deviations, which has the advantage of being symmetric so terms such as $p(z^{(1)}\rightarrow z')/p(z'\rightarrow z^{(1)})=1$. For example, $$p(z^{(1)}\rightarrow z')\sim N(z^{(1)},\sigma^2_{z_{prop}})$$ For testing the algorithm we set proposal standard deviations to 5.\\

For cluster parameters $K,\lambda,\sigma,$ and $\mu$ we do not use MCMC as the posteriors can be sampled directly, as can be seen below.\\




 \textbf{Posteriors:}  For $z,w,\beta$, and $\theta$, the posterior distributions cannot be computed analytically so we use the following relationships for MCMC sampling:
 
 $$\pi(\beta_i|\textbf{X},\textbf{Z},\textbf{W},\theta)\propto \pi(\beta_i)\prod_{k}P(X_{ik}|\beta_i,\theta_k,z_k,w_i)$$  
  $$\pi(\theta_k|\textbf{X},\textbf{Z},\textbf{W},\theta,\sigma_\theta^2)\propto \pi(\theta_k|\sigma_\theta^2)\prod_{i}P(X_{ik}|\beta_i,\theta_k,z_k,w_i)$$ 
   $$\pi(w_i|\textbf{X},\textbf{Z},\textbf{W},\theta,\sigma_w^2)\propto \pi(w_i|\sigma_w^2)\prod_{k}P(X_{ik}|\beta_i,\theta_k,z_k,w_i)$$ 
    $$\pi(z_k|\textbf{X},\textbf{W},\beta,\theta,\sigma_z^2)\propto \pi(z_k|\sigma_z^2)\prod_{i}P(X_{ik}|\beta_i,\theta_k,z_k,w_i)$$ 
    
    For the cluster parameters, the posteriors have closed forms so they can be sampled directly. This method is analogous to that used in Handcock, Raftery, and Tantrum (2007).
    
    $$\mu_{g}|\tx{others}\sim MVN\left(\frac{m_g\overline{z}_g}{m_g+\sigma_g^2/\omega^2},\frac{\sigma_g^2}{m_g+\sigma_g^2/\omega^2}\right)$$ $$\sigma_g^2|\tx{others}\sim (\sigma_0^2+ds_g)\tx{Inv}\chi^2_{\alpha+m_gd}$$ $$P(K_{z_k}=g|\tx{others})=\frac{\lambda_{g}\phi_d(z_k;\mu_{g},\sigma_{g}^2I_d)}{\sum_{g=1}^{G}\lambda_{g}\phi_d(z_k;\mu_{g},\sigma_{g}^2I_d)}$$ $$P(K_{w_i}=g|\tx{others})=\frac{\lambda_{g}\phi_d(w_i;\mu_{g},\sigma_{g}^2I_d)}{\sum_{g=1}^{G}\lambda_{g}\phi_d(w_i;\mu_{g},\sigma_{g}^2I_d)}$$where $$m_g=\sum_{k:K_{z_k}=g}1+\sum_{i:K_{w_i}=g}1$$ $$s_g^2=\frac{1}{d}\left(\sum_{k:K_{z_k}=g}(z_k-\mu_g)^T(z_k-\mu_g)+\sum_{i:K_{w_i}=g}(w_i-\mu_g)^T(w_i-\mu_g)\right)$$ $$\overline{z}_g=\frac{1}{m_g}\left(\sum_{k:K_{z_k}=g}z_k+\sum_{k:K_{z_k}=g}z_k\right)$$ and $\phi_d$ is the d-dimensional multivariate normal density.\\ 
    
    \textbf{MCMC Sampling in the Dirichlet Process Prior Case} Here, for each $i$ and $k$ \\
     
  \textbf{Rejection probabilities} are  $$r(z'_k,z_k^{(t)})=\frac{\pi(z_k'|z_{-k},\beta,\theta,\textbf{W})}{\pi(z_k^{(t)}|z_{-k},\beta,\theta,\textbf{W})}$$ $$r(w'_i,w_i^{(t)})=\frac{\pi(w_i'|w_{-i},\beta,\theta,\textbf{Z})}{\pi(w_i^{(t)}|w_{-i},\beta,\theta,\textbf{Z})}$$
Rejection probabilities are analogous for $\theta$ and $\beta$.\\

\textbf{Overall procedure.} We preform the steps in the following order:

\begin{enumerate}
	\item Propose and update \textbf{z}. 
	\item Propose and update \textbf{w}.
	\item Sample $K,\mu_{g},\sigma_{g}^2,$ and $\lambda_{g}$ from their posterior distributions.
	\item Propose and update $\theta$.
	\item Update $\sigma_\theta^2$ through MCMC.
	\item Propose and update $\beta$.
\end{enumerate}


\end{document}
